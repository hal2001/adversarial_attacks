<style>

.reveal section img {
  border: 0px;
  box-shadow: 0 0 0 0;
}
.reveal table td {
  border: 0px;
}

.reveal table {
  border: 0px;
}

.reveal h1 {
  font-size: 2em;
  hyphens: none;
}

.reveal h3 {
  font-size: 1.2em;
}

.reveal figcaption {
  font-size: 0.4em;
}

.smallcode pre code {
  font-size: 1em;
}

.smallercode pre code {
  font-size: .9em;
}

.reveal .smalltext {
  font-size: 0.75em;
}

.reveal .mediumtext {
  font-size: 0.85em;
}

</style>


I'm a developer, - why should I care about matrices or calculus?
========================================================
author: Sigrid Keydana, Trivadis
date: 12/05/2017
autosize: true
incremental:false
width: 1400
height: 900


About me & my employer
========================================================
class:mediumtext


<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

Trivadis
- DACH-based IT consulting and service company, from traditional technologies to big data/machine learning/data science

My background
- from psychology/statistics via software development and database engineering to data science and ML/DL

My passion
- machine learning and deep learning
- data science and (Bayesian) statistics
- explanation/understanding over prediction accuracy

Where to find me
- blog: http://recurrentnull.wordpress.com
- twitter: @zkajdan


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Part 1: Why should I care about calculus?
</h1>


CIFAR-10 dataset
========================================================

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=12, fig.height=8, fig.path='Figs/',
                      echo=FALSE, warning=FALSE, message=FALSE,
                      cache = TRUE)
options(digits = 3, scipen = 6)
```

```{r}
library(ggplot2)
library(tidyr)
library(dplyr)
library(gridExtra)
library(ggfortify)
library(tensorflow)
library(keras)
K <- keras::backend()
library(EBImage)
library(lars)
source("load_frogs_ships.R")
source("functions.R")
```


10 classes, 32 x 32 px, popular benchmarking dataset for image recognition

&nbsp;

```{r}
layout(matrix(1:80, 8, 10))

for (i in 1:80) {
  img <- EBImage::transpose(Image(data = x_train[i, , , ], colormode = "Color"))
  display(img, method="raster", all = TRUE)
}

```


Let's just pick two classes, keeping it simple...


Ship or frog?
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


&nbsp;

- We've trained a small Convolutional Neural Network (CNN) from scratch to distinguish between ships and frogs, specifically.
- After very few epochs, it tells both classes apart with more than 97% accuracy on the test set.
- Now let's pick some ship ...

```{r,  fig.width=2, fig.height=2}
some_ship <- x_train_combined[5007, , ,  ,drop = FALSE]
some_ship_img <- x_train_combined[5007, , , ] 

layout(1)
plot_as_image(some_ship_img)
```

&nbsp;

... and predict it's class probability:

```{r}
model_name <- "frogs_ships_conv.h5"
model <- load_model_hdf5(model_name)
ship_prob <- model %>% predict_proba(some_ship)
paste("Predicted probability for ship: ", round(ship_prob, 4))
```


So let's make some "random" changes to that image ...
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

... and see what the net says now:

&nbsp;

```{r,fig.width=2, fig.height=2}
library(tensorflow)
library(keras)
K <- keras::backend()

model_name <- "frogs_ships_conv.h5"
model <- load_model_hdf5(model_name)

target <- model %>% predict_classes(some_ship)
target_variable = K$variable(target)

loss <- metric_binary_crossentropy(model$output, target_variable)
gradients <- K$gradients(loss, model$input) 
input <- model$input
output <- model$output
sess <- tf$Session()
sess$run(tf$global_variables_initializer())

evaluated_gradients <- sess$run(gradients,
                                feed_dict = dict(input = some_ship,
                                                 output = model %>% predict_proba(some_ship)))
ship_grads <- evaluated_gradients[[1]]
sgn_ship_grads <- sign(ship_grads)

adv <- some_ship + 0.14 * sgn_ship_grads
adv <- ifelse(adv > 1, 1, adv)
adv <- ifelse(adv < 0, 0, adv)
plot_as_image(adv[1, , , ])

new_ship_prob <- model %>% predict_proba(adv)
print(paste("Predicted probability for ship: ", round(new_ship_prob,4)))
```

&nbsp;



What's going on?
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


&nbsp;


Let's step back from more complicated architectures for a second and look at straightforward logistic regression (with Keras):

&nbsp;

```{r, eval=FALSE, echo=TRUE}
model <- keras_model_sequential()

model %>%
  layer_dense(units = 1, input_shape = 3072) %>%
  layer_activation("sigmoid")

model %>% compile(optimizer = optimizer_sgd(lr = 0.001), loss = "binary_crossentropy")
```


&nbsp;

When does this model say _frog_ (coded as 0) and when _ship_ (coded as 1)?




Logistic regression decision logic
========================================================

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


&nbsp;

Logistic regression uses the _sigmoid_ function to decide which class to output, 0 or 1.

$sigmoid(x,w) = \frac{1}{1 + e^{-\mathbf{w}^T\mathbf{x} + b}}$



```{r, fig.width=8, fig.height=4}
sigmoid <- function(x) 1/(1 + exp(-x))
plot(-10:10, sigmoid(-10:10), type = "l")
```


When $\mathbf{w}^T\mathbf{x} + b$ > 0, sigmoid(x,w) > 0.5, and we get "ship" (1).

When $\mathbf{w}^T\mathbf{x} + b$ < 0, sigmoid(x,w) < 0.5, and we get "frog" (0).


So if we want the algorithm to say "0", for frog ...
========================================================

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


&nbsp;


&nbsp;


$sigmoid(x,w)= \frac{1}{1 + e^{-\mathbf{w}^T\mathbf{x} + b}}$ 

somehow has to end up < 0.5!

&nbsp;

&nbsp;

Let's take a small excerpt of this dot product, $\mathbf{w}^T\mathbf{x}$, and look how we could make it smaller:


$w_1^Tx_1 + w_2^Tx_2 + w_3^Tx_3 ...$

&nbsp;


This will get smaller when for all positive $w_i$, we decrease $x_i$, and for all negative $w_i$, we increase $x_i$.


Let's turn a cute frog into a ship...
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

Here's our frog...

```{r, fig.width=2, fig.height=2}
model_name <- "frogs_ships_logistic.h5"
model <- load_model_hdf5(model_name)

x_train_flat <- x_train_combined %>% apply(1, as.numeric)
x_train_flat <- x_train_flat %>% t()
poor_frog <- x_train_flat[9, , drop = FALSE]
poor_frog_img <- x_train_combined[9, , , ]

plot_as_image(poor_frog_img)
```


To produce a ship, now we want the dot product to get _bigger_.

We extract the weights of the model and add them to the image, scaled by some factor.


```{r, echo=TRUE}
model <- load_model_hdf5(model_name)
weights <- (model %>% get_weights())[[1]]
adv <- poor_frog + t(weights)/6
new_frog_prob <- model %>% predict_proba(adv)
print(paste("Predicted probability for ship: ", round(new_frog_prob,4)))
```

&nbsp;

How does this change the way our image _looks_?

The frog who turned ship (1)
========================================================

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


&nbsp;

```{r, fig.width=2, fig.height=2}
adv_2d <- adv
dim(adv_2d) <- c(32,32,3)
plot_as_image(adv_2d)
```


&nbsp;


The change is hardly visible - much less than with the first example that used a convnet and included nonlinearities.


OK, but that was just simple logistic regression... 
========================================================

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


&nbsp;

... what can we do when we have _deep neural networks_?

And thus: several layers of weights in the game?


<figure>
    <img src='deep_nn.png' width='60%' />
    <figcaption>Source: https://uwaterloo.ca/data-science/sites/ca.data-science/files/uploads/files/lecture_1_0.pdf</figcaption>
</figure>


Training a network, the evil way
========================================================

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

&nbsp;

Normal training has the network _get better_ in doing the _right_ classification.

How about we have it _get worse_, or put differently, have an _alternative definition of what is right_?



Minimizing the cost: Gradient Descent
========================================================

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

In optimization, normally we iteratively _subtract_ (a fraction of) the gradient to reach the/a minimum of the cost function.


In this graphic, cost is a quadratic function, e.g., _mean squared error_:

&nbsp;

<figure>
    <img src='convex.png' width='50%'/>
     <figcaption>Source: Goodfellow et al. 2016, <a href='http://www.deeplearningbook.org/'>Deep Learning</a>
</figure>


Loss for logistic regression
========================================================

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

In two-class classification (be it shallow logistic regression or a deep convnet), the loss to minimize is cross entropy:

$- \sum_j{t_j log(y_j)}$

resp.

$- (t\:log(y) + (1-t)\:log(1-y))$ for the binary case.

&nbsp;


Normally we'd try to minimize that loss ...


Being evil
========================================================

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

... now we _maximize_ the error instead of minimizing it!

&nbsp;

Let's step back for a second and see this how this works in context.



What a neural network needs for learning
========================================================

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

- a cost function: How different is my prediction from my target?
- a way to learn (optimization): How do I adapt my _weights_ such that my predictions get better?
- a way to back propagate the cost: How do weights _earlier_ in the network change the predictions (and thus, the cost)?

&nbsp;

Enter ...


<figure>
    <img src='deep_nn.png' width='20%' />
    <figcaption>Source: https://uwaterloo.ca/data-science/sites/ca.data-science/files/uploads/files/lecture_1_0.pdf</figcaption>
</figure>

Backpropagation!
========================================================

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

- basically, just the chain rule: $\frac{dz}{dx} = \frac{dz}{dy} \frac{dy}{dx}$
- chained over several layers:

&nbsp;

<figure>
    <img src='backprop2.png' width='60%'/>
    <figcaption>Source: <a href=https://colah.github.io/posts/2015-08-Backprop/>https://colah.github.io/posts/2015-08-Backprop/</a></figcaption>
</figure>


Learning weights by backpropagation (1)
========================================================

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp; 

We'll gradually build up the gradient of the loss with respect to $y_i$, the output of the last hidden layer, and the weights $w_{ij}$, respectively.


Here 
- $i$ and $j$ are layers of the network ($j$ being the output layer)
- $y_l$ is the output of layer $l$
- $z_l$ is the aggregated input going into layer $l$ (before the activation function is applied)

<figure>
    <img src='backprop4.png' width="20%"/>
</figure>

&nbsp;


Learning weights by backpropagation (2)
========================================================

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />

&nbsp; 

<figure>
    <img src='backprop4.png' width="15%"/>
</figure>

&nbsp;

__Step 1: Loss w.r.t. output (prediction)__

At the output layer $j$, we compare class prediction $y_j$ and actual class $t$, using binary cross entropy/logistic loss: 
$- (t\:log(y) + (1-t)\:log(1-y))$ 

The gradient 

$\frac{dE}{dy_j} = - (\frac{t}{y} + (-1) \frac{1-t}{1-y}) = \frac{y-t}{y(1-y)}$ 

describes how the error $E$ changes as the prediction $y_j$ changes. 
   

Learning weights by backpropagation (2)
========================================================

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp; 

<figure>
    <img src='backprop4.png' width="15%"/>
</figure>

&nbsp;
  
__Step 2: How does the prediction/output $y_j$ change as the input $z_j$ to the final neuron changes?__

In the case of a logistic (sigmoid) neuron with output $y_j$, this is described by the gradient

$\frac{dy_j}{dz_j} = y_j(1-y_j)$.

 
Learning weights by backpropagation (3)
========================================================

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp; 

<figure>
    <img src='backprop4.png' width="15%"/>
</figure>

&nbsp;
  
__Step 3a: How does the input $z_j$ to the final layer change as the weight $w_{ij}$ changes?__

Here the gradient is 

$\frac{dz_j}{dw_{ij}} = y_i$, 

that is, the output of layer $i$.

 
Learning weights by backpropagation (4)
========================================================

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp; 

<figure>
    <img src='backprop4.png' width="15%"/>
</figure>

&nbsp;
  
__Step 3b: How does the input $z_j$ to the final layer change as the output of layer $i$ changes?__

Here we have to take into account all connections a neuron $y_i$ has to the output layer: The gradient is 

$\sum_j \frac{dz_j}{dy_i} = \sum_j w_{ij}$,

that is, the sum of the weights going out of $y_i$.

 
   
Learning weights by backpropagation: Putting it all together
========================================================

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp; 

<figure>
    <img src='backprop4.png' width="15%"/>
</figure>

&nbsp;
  
__Now that we have the single gradients, we use the _chain rule_ to back propagate the error:__

The gradient of the loss w.r.t. $y_i$ is

$\frac{dE}{dy_i} = \frac{y-t}{y(1-y)} y_j(1-y_j) \sum_j w_{ij}$

Accordingly, we get the gradient of the loss w.r.t. $w_{ij}$ as

$\frac{dE}{dy_i} = \frac{y-t}{y(1-y)} y_j(1-y_j) y_i$




Backprop, the evil way...
========================================================

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

We can use the same mechanics of loss calculation and backpropagation, just

- we change the input, not the weights
- we _maximize_ the loss by _adding_, not subtracting (some fraction of) the gradient 


&nbsp;

So instead of the weights, we'll extract the gradient from the model and add part of it to the input image.

How do we find the best scaling factor?

Enter: Fast Gradient Sign Method (FGSM)
========================================================

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


&nbsp;

As shown in Goodfellow et al.,

<a href="https://arxiv.org/pdf/1412.6572.pdf">Explaining and harnessing adversarial examples</a>,

we can just take the sign of the gradient, scale it by a factor $\epsilon$, reflecting the "resolution" of the measurements, and add that to the input image:


&nbsp;

$x = x + \epsilon \: sign(\nabla x (J(\theta, x ,y)))$


Frog FGSM
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


&nbsp;

Here's our frog again ...

```{r, fig.width=2, fig.height=2}
x_train_flat <- x_train_combined %>% apply(1, as.numeric)
x_train_flat <- x_train_flat %>% t()
poor_frog <- x_train_flat[9, , drop = FALSE]
poor_frog_img <- x_train_combined[9, , , ]

plot_as_image(poor_frog_img)
library(keras)
K <- keras::backend()
model_name <- "frogs_ships_logistic.h5"; model <- load_model_hdf5(model_name)
```


This time we get the gradient values, take their sign, and add that to the image, scaled by a factor $\epsilon$:

```{r, echo=TRUE}
input <- model$input; output <- model$output
target <- model %>% predict_classes(poor_frog); target_variable = K$variable(target)
loss <- metric_binary_crossentropy(model$output, target_variable)
gradients <- K$gradients(loss, model$input) 
get_grad_values <- K$`function`(list(model$input), gradients)
grads <- get_grad_values(list(poor_frog))[[1]] %>% sign()

adv <- poor_frog + grads * 0.02
new_frog_prob <- model %>% predict_proba(adv)
print(paste("Predicted probability for ship: ", round(new_frog_prob,4)))
```

And how does our frog look now?


The frog who turned ship (2)
========================================================

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />


&nbsp;

```{r, fig.width=2, fig.height=2}
adv_2d <- adv
dim(adv_2d) <- c(32,32,3)
plot_as_image(adv_2d)
```


========================================================
type:prompt

&nbsp; 

&nbsp; 

<h1>
Part 2: Why should I care about matrices?
</h1>


Everything is a matrix (or tensor)
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

Images are.

&nbsp;

Our little

```{r, fig.width=2, fig.height=2}
poor_frog <- x_train_combined[9, , ,  ,drop = FALSE]
poor_frog_img <- x_train_combined[9, , , ] 
plot_as_image(poor_frog_img)
```

is just coded like this...

```{r}
poor_frog[1, 1:8, 1:8 ,2]
```



Everything is a matrix (or tensor)
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

Documents are.

&nbsp;

In text classification, we have term-document matrices

```{r}
df <- data.frame(
  doc1 = c(2,0,0,0,1,1,0,0,0,1,2,0,0,0,1,1,0,0,0,1),
  doc2 = c(1,2,0,0,2,0,0,0,0,0,2,0,0,0,1,1,0,0,0,1),
  doc3 = c(1,2,0,0,2,0,0,0,0,0,2,0,0,0,1,1,0,0,0,1),
  doc4 = c(1,1,0,0,2,0,0,2,0,0,2,0,0,0,1,1,0,0,0,1),
  doc5 = c(0,0,0,0,2,0,0,2,0,0,2,0,0,0,1,1,0,0,0,1),
  doc6 = c(0,2,0,0,2,0,0,1,0,0,2,0,0,0,1,1,0,0,0,1),
  doc7 = c(0,2,0,0,2,0,0,2,0,0,2,0,0,0,1,1,0,0,0,1),
  doc8 = c(0,2,0,0,2,0,0,1,0,0,2,0,0,0,1,1,0,0,0,1),
  doc9 = c(1,2,0,0,2,0,0,0,0,0,2,0,0,0,1,1,0,0,0,1)
)
rownames(df) <- c("word1", "word2", "word3", "word4", "word5", "word6", "word7", "word8", "word9", "word10", "word11", "word12", "word13", "word14", "word15", "word16", "word17", "word18", "word19","word20")
df
```


Everything is a matrix (or tensor)
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

Even normal dataframes are.

<figure>
    <img src='iris.jpg' width='20%'/>
</figure>

```{r}
iris[1:10, ]
```


First things first: Matrix multiplication (1)
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

Let's say we're building our own neural network, starting with a 8*4 input matrix X and a hidden layer of capacity (number of neurons) 4.

```{r, echo=TRUE}
X <- matrix(1:32, nrow = 8, ncol = 4, byrow = TRUE)
X
```

&nbsp;

What shape do we need for the weights matrix going into that hidden layer?

First things first: Matrix multiplication (2)
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

Weights matrix is 4*4.

```{r, echo=TRUE}
W <- matrix(rnorm(16), nrow = 4, ncol = 4)
```

&nbsp;

Then matrix-multiplying X and W gives the aggregated inputs into each hidden neuron for each input sample.

```{r, echo=TRUE}
X %*% W
```


The magic behind convnets: Convolutions
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;


LeNet: First successful application of convolutional neural networks by Yann LeCun, Yoshua Bengio et al.

<figure>
    <img src='lenet.png' width='80%'/>
    <figcaption>Source: http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf</figcaption>
</figure>



The convolution operation
========================================================

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

<figure>
    <img src='2dconv.png' width='60%'/>
    <figcaption>Source: Goodfellow et al., Deep Learning.</figcaption>
</figure>


Convolution as matrix multiplication (1)
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

We have an input image x ... 

```{r}
(x <- matrix(1:9, nrow = 3, byrow = TRUE))
```

and a convolution kernel k...

```{r}
(k <- matrix(c(1,2,3,4), nrow = 2, byrow = TRUE))
```

We transform k into a _doubly block-circulant matrix_ ...

```{r}
(k <- matrix(c(1,2,0,3,4,0,0,0,0,
              0,1,2,0,3,4,0,0,0,
              0,0,0,1,2,0,3,4,0,
              0,0,0,0,1,2,0,3,4),
            nrow = 4,
            byrow = TRUE))
```

and flatten x into a vector...

```{r}
(x<-1:9)
```


Convolution as matrix multiplication (2)
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

Now we can get the convolved image by matrix-multiplying k and x:

```{r, echo=TRUE}
k %*% x
```


This is the same we get when performing the convolution "manually":

```{r, echo=TRUE, results="hide"}
y1 <- 1*1 + 2*2 + 3*4 + 4*5
y2 <- 1*2 + 2*3 + 3*5 + 4*6
y3 <- 1*4 + 2*5 + 3*7 + 4*8
y4 <- 1*5 + 2*6 + 3*8 + 4*9
```


Now just convert the vector back to a matrix:

```{r}
(y <- matrix(c(y1,y2,y3,y4), nrow = 2, byrow = TRUE))
```

The hidden magic: Matrix decompositions
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

_QR, LU, SVD, NMF, eigen..._

&nbsp;

Motivations

- semantic: find latent factors/components  
    - topics (e.g., in document classification)
    - features (e.g., in image or audio data)

- technical: facilitate computations (e.g., Least Squares)

```{r, echo=TRUE, results="hide"}
?lm
```

```
lm(formula, data, subset, weights, na.action,
   method = "qr", model = TRUE, x = FALSE, y = FALSE, qr = TRUE,
   singular.ok = TRUE, contrasts = NULL, offset, ...)
```

Applications: How it works
========================================================

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

&nbsp;

<figure>
    <img src='factorisation.png' width='60%' />
    <figcaption>Source: <a href="http://perso.telecom-paristech.fr/~essid/teach/NMF_tutorial_ICME-2014.pdf" >Essid, S. & Ozerov, A.: A tutorial on nonnegative matrix factorisation with applications to audiovisual content analysis</a></figcaption>
</figure>


Finding topics in documents
========================================================

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

&nbsp;

<figure>
    <img src='topics.png' width='70%' />
    <figcaption>Source: <a href="http://perso.telecom-paristech.fr/~essid/teach/NMF_tutorial_ICME-2014.pdf" >Essid, S. & Ozerov, A.: A tutorial on nonnegative matrix factorisation with applications to audiovisual content analysis</a></figcaption>
</figure>


Finding "facial features"
========================================================

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

&nbsp;

<figure>
    <img src='facialfeatures.png' width='60%' />
    <figcaption>Source: <a href="http://perso.telecom-paristech.fr/~essid/teach/NMF_tutorial_ICME-2014.pdf" >Essid, S. & Ozerov, A.: A tutorial on nonnegative matrix factorisation with applications to audiovisual content analysis</a></figcaption>
</figure>


Soft clustering
========================================================

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

&nbsp;

<figure>
    <img src='clustering.png' width='60%' />
    <figcaption>Source: <a href="http://perso.telecom-paristech.fr/~essid/teach/NMF_tutorial_ICME-2014.pdf" >Essid, S. & Ozerov, A.: A tutorial on nonnegative matrix factorisation with applications to audiovisual content analysis</a></figcaption>
</figure>


Technical: Matrix factorizations for least squares
========================================================

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

Linear regression $\hat y = \alpha + \beta_1 x_1 + \beta_2 x_2 ... + \beta_n x_n$

has a naive solution using the _normal equations_

$A^TAx= A^Tb$    &nbsp; &nbsp; (from $A^T(b - A \hat x) = 0)$)

which means we need to find the inverse $(A^TA)^{-1}$ to get $x$.
<figure>
    <img src='ls.png' width='40%' />
    <figcaption>Source:<a href=" http://www.stat.wisc.edu/~ifischer/Intro_Stat/Lecture_Notes/APPENDIX/A2._Geometric_Viewpoint/A2.3_-_Least_Squares_Approximation.pdf">
    http://www.stat.wisc.edu/~ifischer/Intro_Stat/Lecture_Notes/APPENDIX/A2._Geometric_Viewpoint/A2.3_-_Least_Squares_Approximation.pdf
    </a>
    </figcaption>
</figure>

So what's bad about taking the inverse?
========================================================

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

- slow
- numerical instability
- sparse matrices have dense inverses (most of the time)


Least Squares via matrix inverse
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

```{r, echo=TRUE}
data(diabetes)
X <- diabetes$x
y <- diabetes$y
X_with_intercept <- cbind(rep(1, nrow(X)), X)

(beta_hat <- solve(t(X_with_intercept) %*% X_with_intercept) %*% t(X_with_intercept) %*% y)
```



Least Squares via QR decomposition
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;

<figure>
    <img src='qr.png' width='60%' />
    <figcaption>Source:<a href=" http://www.sharetechnote.com/html/Handbook_EngMath_Matrix_QRDecomposition.html">
    http://www.sharetechnote.com/html/Handbook_EngMath_Matrix_QRDecomposition.html
    </a>
    </figcaption>
</figure>

```{r, echo=TRUE}
# A = QR  
# QRx = b    ### Q inverse = Q transpose
# Rx = Q'b   ### x = backsubstitute(R, Q'b)

qr_solve <- function(A, b) {
  A_qr <- qr(A)
  R <- qr.R(A_qr)
  Q <- qr.Q(A_qr)
  backsolve(R, t(Q) %*% b)
}
```


The end, kind of
========================================================
class:smallcode

<img src="tri_logo_high.jpg" style="position:absolute;top:0px;right:0px; width: 10%" />
<img src='cube3.png' border=0 style="position:absolute;top:90%;right:0px; width: 8%" />

&nbsp;


Hope I've been successful a bit in arousing your curiosity about calculus and linear algebra :-)

There's a lot you could do without, but it'd be half the fun ;-)

&nbsp;

&nbsp;

Questions?

Thanks for your attention!!


